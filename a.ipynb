{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad66c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import magic\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "openai_api_key = *************\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# pip install unstructured\n",
    "# Other dependencies to install https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html\n",
    "# pip install python-magic-bin\n",
    "# pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8a28a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your loader ready\n",
    "loader = DirectoryLoader('F:\\\\EDUCATION\\\\PROJECTS\\\\Wise_My_GPT\\\\Law', glob='**\\\\*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a9740d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\radee/nltk_data'\n    - 'f:\\\\EDUCATION\\\\PROJECTS\\\\.conda\\\\nltk_data'\n    - 'f:\\\\EDUCATION\\\\PROJECTS\\\\.conda\\\\share\\\\nltk_data'\n    - 'f:\\\\EDUCATION\\\\PROJECTS\\\\.conda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\radee\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load up your text into documents\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m documents \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\langchain\\document_loaders\\directory.py:131\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m items:\n\u001b[1;32m--> 131\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_file(i, p, docs, pbar)\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m pbar:\n\u001b[0;32m    134\u001b[0m     pbar\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\langchain\\document_loaders\\directory.py:92\u001b[0m, in \u001b[0;36mDirectoryLoader.load_file\u001b[1;34m(self, item, path, docs, pbar)\u001b[0m\n\u001b[0;32m     90\u001b[0m         logger\u001b[39m.\u001b[39mwarning(e)\n\u001b[0;32m     91\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     93\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m pbar:\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\langchain\\document_loaders\\directory.py:86\u001b[0m, in \u001b[0;36mDirectoryLoader.load_file\u001b[1;34m(self, item, path, docs, pbar)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m _is_visible(item\u001b[39m.\u001b[39mrelative_to(path)) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_hidden:\n\u001b[0;32m     85\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m         sub_docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_cls(\u001b[39mstr\u001b[39;49m(item), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_kwargs)\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m     87\u001b[0m         docs\u001b[39m.\u001b[39mextend(sub_docs)\n\u001b[0;32m     88\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\langchain\\document_loaders\\unstructured.py:71\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m     70\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[0;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     73\u001b[0m         docs: List[Document] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\langchain\\document_loaders\\unstructured.py:156\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[0;32m    154\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m partition\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\unstructured\\partition\\auto.py:201\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, ssl_verify, ocr_languages, pdf_infer_table_structure, xml_keep_tags, data_source_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     elements \u001b[39m=\u001b[39m partition_image(\n\u001b[0;32m    192\u001b[0m         filename\u001b[39m=\u001b[39mfilename,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         file\u001b[39m=\u001b[39mfile,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    199\u001b[0m     )\n\u001b[0;32m    200\u001b[0m \u001b[39melif\u001b[39;00m filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mTXT:\n\u001b[1;32m--> 201\u001b[0m     elements \u001b[39m=\u001b[39m partition_text(\n\u001b[0;32m    202\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[0;32m    203\u001b[0m         file\u001b[39m=\u001b[39;49mfile,\n\u001b[0;32m    204\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    205\u001b[0m         paragraph_grouper\u001b[39m=\u001b[39;49mparagraph_grouper,\n\u001b[0;32m    206\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    207\u001b[0m     )\n\u001b[0;32m    208\u001b[0m \u001b[39melif\u001b[39;00m filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mRTF:\n\u001b[0;32m    209\u001b[0m     elements \u001b[39m=\u001b[39m partition_rtf(\n\u001b[0;32m    210\u001b[0m         filename\u001b[39m=\u001b[39mfilename,\n\u001b[0;32m    211\u001b[0m         file\u001b[39m=\u001b[39mfile,\n\u001b[0;32m    212\u001b[0m         include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks,\n\u001b[0;32m    213\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    214\u001b[0m     )\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\unstructured\\documents\\elements.py:214\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 214\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    215\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    216\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:531\u001b[0m, in \u001b[0;36madd_metadata_with_filetype.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m    530\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 531\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    532\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    533\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\unstructured\\partition\\text.py:128\u001b[0m, in \u001b[0;36mpartition_text\u001b[1;34m(filename, file, text, encoding, paragraph_grouper, metadata_filename, include_metadata, max_partition, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m     file_text \u001b[39m=\u001b[39m group_broken_paragraphs(file_text)\n\u001b[1;32m--> 128\u001b[0m file_content \u001b[39m=\u001b[39m split_by_paragraph(file_text, max_partition\u001b[39m=\u001b[39;49mmax_partition)\n\u001b[0;32m    130\u001b[0m elements: List[Element] \u001b[39m=\u001b[39m []\n\u001b[0;32m    131\u001b[0m metadata \u001b[39m=\u001b[39m (\n\u001b[0;32m    132\u001b[0m     ElementMetadata(filename\u001b[39m=\u001b[39mmetadata_filename \u001b[39mor\u001b[39;00m filename)\n\u001b[0;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m include_metadata\n\u001b[0;32m    134\u001b[0m     \u001b[39melse\u001b[39;00m ElementMetadata()\n\u001b[0;32m    135\u001b[0m )\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\unstructured\\partition\\text.py:37\u001b[0m, in \u001b[0;36msplit_by_paragraph\u001b[1;34m(content, max_partition)\u001b[0m\n\u001b[0;32m     34\u001b[0m split_paragraphs \u001b[39m=\u001b[39m []\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m paragraph \u001b[39min\u001b[39;00m paragraphs:\n\u001b[0;32m     36\u001b[0m     split_paragraphs\u001b[39m.\u001b[39mextend(\n\u001b[1;32m---> 37\u001b[0m         _split_to_fit_max_content(paragraph, max_partition\u001b[39m=\u001b[39;49mmax_partition),\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m split_paragraphs\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\unstructured\\partition\\text.py:54\u001b[0m, in \u001b[0;36m_split_to_fit_max_content\u001b[1;34m(content, max_partition)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_to_fit_max_content\u001b[39m(content: \u001b[39mstr\u001b[39m, max_partition: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1500\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m     52\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Splits a section of content so that all of the elements fit into the\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    max partition window.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     sentences \u001b[39m=\u001b[39m sent_tokenize(content)\n\u001b[0;32m     55\u001b[0m     num_sentences \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sentences)\n\u001b[0;32m     57\u001b[0m     chunks \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\unstructured\\nlp\\tokenize.py:38\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39mCACHE_MAX_SIZE)\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m     37\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A wrapper around the NLTK sentence tokenizer with LRU caching enabled.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m _sent_tokenize(text)\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mf:\\EDUCATION\\PROJECTS\\.conda\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\radee/nltk_data'\n    - 'f:\\\\EDUCATION\\\\PROJECTS\\\\.conda\\\\nltk_data'\n    - 'f:\\\\EDUCATION\\\\PROJECTS\\\\.conda\\\\share\\\\nltk_data'\n    - 'f:\\\\EDUCATION\\\\PROJECTS\\\\.conda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\radee\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Load up your text into documents\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3153f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your text splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a792c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your documents into texts\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cad0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn your texts into embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734ed265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your docsearch ready\n",
    "docsearch = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66826924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up your LLM\n",
    "llm = OpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817a0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your Retriever\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b81063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' McCarthy discovered that a programming language could be constructed from a handful of simple operators and a notation for functions, using a data structure called a list for both code and data.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a query\n",
    "query = \"what is Homicide, what are the variations ?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22231c5",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694343cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                chain_type=\"stuff\",\n",
    "                                retriever=docsearch.as_retriever(),\n",
    "                                return_source_documents=True)\n",
    "query = \"What did McCarthy discover?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bec53323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' McCarthy discovered a way to build a whole programming language using a handful of simple operators and a notation for functions. He called this language Lisp, for \"List Processing,\" because one of his key ideas was to use a simple data structure called a list for both code and data.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32246ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='May 2001\\n\\n(I wrote this article to help myself understand exactly\\n\\nwhat McCarthy discovered.  You don\\'t need to know this stuff\\n\\nto program in Lisp, but it should be helpful to\\n\\nanyone who wants to\\n\\nunderstand the essence of Lisp \\x97 both in the sense of its\\n\\norigins and its semantic core.  The fact that it has such a core\\n\\nis one of Lisp\\'s distinguishing features, and the reason why,\\n\\nunlike other languages, Lisp has dialects.)In 1960, John\\n\\nMcCarthy published a remarkable paper in\\n\\nwhich he did for programming something like what Euclid did for\\n\\ngeometry. He showed how, given a handful of simple\\n\\noperators and a notation for functions, you can\\n\\nbuild a whole programming language.\\n\\nHe called this language Lisp, for \"List Processing,\"\\n\\nbecause one of his key ideas was to use a simple\\n\\ndata structure called a list for both\\n\\ncode and data.It\\'s worth understanding what McCarthy discovered, not\\n\\njust as a landmark in the history of computers, but as', metadata={'source': '../data/PaulGrahamEssaySmall/rootsoflisp.txt'}),\n",
       " Document(page_content=\"itself.  To understand what McCarthy meant by this,\\n\\nwe're going to retrace his steps, with his mathematical\\n\\nnotation translated into running Common Lisp code.\", metadata={'source': '../data/PaulGrahamEssaySmall/rootsoflisp.txt'}),\n",
       " Document(page_content=\"a model for what programming is tending to become in\\n\\nour own time.  It seems to me that there have been\\n\\ntwo really clean, consistent models of programming so\\n\\nfar: the C model and the Lisp model.\\n\\nThese two seem points of high ground, with swampy lowlands\\n\\nbetween them.  As computers have grown more powerful,\\n\\nthe new languages being developed have been moving\\n\\nsteadily toward the Lisp model.  A popular recipe\\n\\nfor new programming languages in the past 20 years\\n\\nhas been to take the C model of computing and add to\\n\\nit, piecemeal, parts taken from the Lisp model,\\n\\nlike runtime typing and garbage collection.In this article I'm going to try to explain in the\\n\\nsimplest possible terms what McCarthy discovered.\\n\\nThe point is not just to learn about an interesting\\n\\ntheoretical result someone figured out forty years ago,\\n\\nbut to show where languages are heading.\\n\\nThe unusual thing about Lisp \\x97 in fact, the defining\\n\\nquality of Lisp \\x97 is that it can be written in\", metadata={'source': '../data/PaulGrahamEssaySmall/rootsoflisp.txt'}),\n",
       " Document(page_content=\"January 2023(Someone fed my essays into GPT to make something that could answer\\n\\nquestions based on them, then asked it where good ideas come from.  The\\n\\nanswer was ok, but not what I would have said. This is what I would have said.)The way to get new ideas is to notice anomalies: what seems strange,\\n\\nor missing, or broken? You can see anomalies in everyday life (much\\n\\nof standup comedy is based on this), but the best place to look for\\n\\nthem is at the frontiers of knowledge.Knowledge grows fractally.\\n\\nFrom a distance its edges look smooth, but when you learn enough\\n\\nto get close to one, you'll notice it's full of gaps. These gaps\\n\\nwill seem obvious; it will seem inexplicable that no one has tried\\n\\nx or wondered about y. In the best case, exploring such gaps yields\\n\\nwhole new fractal buds.\", metadata={'source': '../data/PaulGrahamEssaySmall/getideas.txt'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
